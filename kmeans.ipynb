{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_toolkits.basemap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-118523ac63e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limiter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRateLimiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits.basemap'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.vq import kmeans2, whiten\n",
    "from sklearn.cluster import KMeans\n",
    "import geopy\n",
    "import zipcodes\n",
    "from time import sleep\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot pick-up data using k-means clustering\n",
    "def plotKMeans(lat, long, k, iters):\n",
    "    coordinates= np.array(list(zip(lat, long)))\n",
    "    centroids, labels = kmeans2(whiten(coordinates), k, iter = iters)  \n",
    "    plt.scatter(coordinates[:,0], coordinates[:,1], c=labels);\n",
    "    plt.show()\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDiGraphK(data, sample_size, k=20, iters=20, name='ktemp'):\n",
    "    # cluster data using k-means\n",
    "    lat = data['Lat']\n",
    "    lon = data['Lon']\n",
    "    coordinates= np.array(list(zip(lat, lon)))\n",
    "    print(coordinates)\n",
    "    centroids, labels = kmeans2(whiten(coordinates), k, iter = iters) \n",
    "    # create graph with k nodes\n",
    "    G = nx.DiGraph(); \n",
    "    for pick_up in range(sample_size):\n",
    "        \n",
    "        # get k-means centroid from src and randomize dest based on uniform distribution among all clusters\n",
    "        #rand_idx = random.randint(0, len(labels) - 1)\n",
    "        rand_idx = pick_up\n",
    "        la = lat[pick_up]\n",
    "        lo = lon[pick_up]\n",
    "        src = labels[rand_idx]\n",
    "        dest = random.randint(0, k - 1)\n",
    "        if (dest == src and src != 0):\n",
    "            dest = 0\n",
    "\n",
    "        # increase edge weight from src -->  dest by 1 \n",
    "        if not G.has_node(src):\n",
    "            G.add_node(src, lat=0, lon=0)\n",
    "        if not G.has_node(dest):\n",
    "            G.add_node(dest, lat=0, lon=0)\n",
    "        if not G.has_edge(src, dest):\n",
    "            G.add_edge(src, dest, weight=0)\n",
    "        G[src][dest]['weight'] += 1\n",
    "        G.node[src]['lat'] = (G.node[src]['lat'] +  la) / 2 #keep a running average\n",
    "        G.node[src]['lon'] =  (G.node[src]['lon'] +  lo) / 2\n",
    "        \n",
    "        #print(\"Adding edge from \" + str(src) + \" to \" + str(dest))\n",
    "        if (pick_up > 1000 and pick_up % 1000 == 0):\n",
    "            nx.write_graphml(G, \"graphs/\" + name + \".graphml\")\n",
    "    return G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to files\n",
    "path_uber_raw = \"uber-trip-data/uber-raw-data-apr14.csv\"\n",
    "data = pd.read_csv(path_uber_raw)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot pick-up data using k-means clustering\n",
    "def plotKMeans(lat, long, k, iters):\n",
    "    coordinates= np.array(list(zip(lat, long)))\n",
    "    centroids, labels = kmeans2(whiten(coordinates), k, iter = iters)  \n",
    "    plt.scatter(coordinates[:,0], coordinates[:,1], c=labels);\n",
    "    plt.show()\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = data['Lat']\n",
    "lon = data['Lon']\n",
    "lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_basemap(lat, lon, coords, shapefiles={}):\n",
    "    m = Basemap(projection='merc',llcrnrlat=coords[0],urcrnrlat=coords[1],\\\n",
    "            llcrnrlon=coords[2], urcrnrlon=coords[3], resolution='h')\n",
    "    m.drawcoastlines()\n",
    "    m.drawstates()\n",
    "    m.fillcontinents(color='coral',lake_color='aqua')\n",
    "    for file, name in shapefiles.items():\n",
    "        m.readshapefile(file, name)\n",
    "    x,y = m(lon, lat)\n",
    "    m.plot(x,y, 'bo', markersize=1)\n",
    "    plt.show()\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, l = plotKMeans(lat[1000:1100], lon[10000:10090], 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup mercator basemap.\n",
    "lat = data['Lat'].tolist()\n",
    "lon = data['Lon'].tolist()\n",
    "man_coords = [40.6664, 40.8851, -74.0587, -73.8890]\n",
    "shapefiles ={}\n",
    "shapefiles['shapes/ZIP_CODE_N'] = 'zipcodes'\n",
    "m1=draw_basemap(lat[1:100], lon[1:100], man_coords, shapefiles) # plots raw points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_small = createDiGraphK(data, 2000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos(G, m):\n",
    "    pos = {}\n",
    "    #lat = {k:mean(v) for (k,v) in nx.get_node_attributes(test1, 'lat').items()}\n",
    "    #lon = {k:mean(v) for (k,v) in nx.get_node_attributes(test1, 'lon').items()}\n",
    "    lt = nx.get_node_attributes(G, 'lat')\n",
    "    ln = nx.get_node_attributes(G, 'lon')\n",
    "    for node_num in list(G.nodes()):\n",
    "        pos[node_num] = m(ln[node_num], lt[node_num]) # KEEP IN MIND THIS IS NOW X,Y\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_graph(G, coords=[40.58, 40.88, -74.28, -73.840], shapefiles={}, node_size=50): \n",
    "    plt.figure(figsize=(18,12))\n",
    "    m = Basemap(projection='merc',llcrnrlat=coords[0],urcrnrlat=coords[1],llcrnrlon=coords[2], urcrnrlon=coords[3], resolution='h')\n",
    "    m.drawcoastlines()\n",
    "    m.drawstates()\n",
    "    m.fillcontinents(color='lightgreen',lake_color='aqua')\n",
    "    #m = Basemap(projection='merc', width=10, height=10, lat_0=40.783058, lon_0=-73.971252)\n",
    "    pos = get_pos(G, m)\n",
    "    print(\"FINISHED PROJ. COORDINATES...STARTING NODES\")\n",
    "    nx.draw_networkx_nodes(G = G, pos = pos, node_color = 'b', alpha=0.8,node_size=node_size)\n",
    "    nx.draw_networkx_edges(G = G, pos = pos, edge_color='white', alpha=0.8, arrows = True)\n",
    "    for file, name in shapefiles.items():\n",
    "        m.readshapefile(file, name)\n",
    "    plt.show()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shapefiles ={}\n",
    "shapefiles['shapes/ZIP_CODE_N'] = 'zipcodes'\n",
    "man_coords = [40.58, 40.88, -74.28, -73.840] #manhattan by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_all = createDiGraphK(data, len(data), k=30, iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(test_all, man_coords, shapefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale graph \n",
    "node_lt = nx.get_node_attributes(test_all, 'lat').values()\n",
    "node_ln = nx.get_node_attributes(test_all, 'lon').values()\n",
    "pad = 0.01\n",
    "c2 = [min(node_lt)-pad, max(node_lt)+pad, min(node_ln)-pad, max(node_ln)+pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = draw_graph(test_all, c2, shapefiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yay we got it visualized! now let's try to make the graph more meaningful by incorporating some of measures of centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats(G):\n",
    "    stats = {};  \n",
    "    stats['out_degrees'] = G.out_degree(weight='weight'); \n",
    "    stats['closeness_centrality'] = nx.closeness_centrality(G); # hard to do the rest of these because of uniform dist. \n",
    "    stats['betweenness_centrality'] = nx.betweenness_centrality(G); \n",
    "    stats['eigenvalue_centrality'] = nx.eigenvector_centrality(G); \n",
    "    stats['pagerank'] = nx.pagerank(G); \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all.out_degree(weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust sizing of nodes based on their relative out-degrees\n",
    "og_size =  400\n",
    "sizes = []\n",
    "for node in list(test_all.nodes()):\n",
    "    sizes.append(og_size*0.5*(out_weights[node])/2.0)\n",
    "draw_graph(test_all, c2, shapefiles, node_size=sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_nodes(G, og_size):\n",
    "    # get weights into the range [0.5-1]\n",
    "    out_raw = dict(G.out_degree(weight='weight'))\n",
    "    min_out = min(out_raw.values())\n",
    "    max_out = max(out_raw.values())\n",
    "    out_weights = {k:(v-min_out)/(max_out-min_out) for (k,v) in out_raw.items()}\n",
    "    \n",
    "    # adjust sizing of nodes based on their relative out-degrees\n",
    "    sizes = []\n",
    "    for node in list(G.nodes()):\n",
    "        sizes.append(og_size*0.5*(out_weights[node]) + 0.5*og_size)\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm so there's BARELY any usage of the outer nodes. let's try to remove some outliers...and add a function to scale the map if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove outliers \n",
    "def removeOutliers(x, outlierConstant):\n",
    "    a = np.array(x)\n",
    "    upper_quartile = np.percentile(a, 75)\n",
    "    lower_quartile = np.percentile(a, 25)\n",
    "    IQR = (upper_quartile - lower_quartile) * outlierConstant\n",
    "    quartileSet = (lower_quartile - IQR, upper_quartile + IQR)\n",
    "    resultList = []\n",
    "    for y in a.tolist():\n",
    "        if y >= quartileSet[0] and y <= quartileSet[1]:\n",
    "            resultList.append(y)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return indices of valid points-- SHOULD USE THIS INSTEAD WHEN CONSIDERING TWO RELATED SETS OF POINTS!!!\n",
    "def getOutliers(x, y, outlierConstant):\n",
    "    a = np.array(x)\n",
    "    b = np.array(y)\n",
    "    upper_quartile_a = np.percentile(a, 75)\n",
    "    lower_quartile_a = np.percentile(a, 25)\n",
    "    upper_quartile_b = np.percentile(b, 75)\n",
    "    lower_quartile_b = np.percentile(b, 25)\n",
    "    IQR_a = (upper_quartile_a - lower_quartile_a) * outlierConstant\n",
    "    IQR_b = (upper_quartile_b - lower_quartile_b) * outlierConstant\n",
    "    quartileSet_a = (lower_quartile_a - IQR_a, upper_quartile_a + IQR_a)\n",
    "    quartileSet_b = (lower_quartile_b - IQR_b, upper_quartile_b + IQR_b)\n",
    "    valid = []\n",
    "    for i,v in enumerate(a.tolist()):\n",
    "        if (v >= quartileSet_a[0] and v <= quartileSet_a[1] and b[i] >= quartileSet_b[0] and b[i] <= quartileSet_b[1]):\n",
    "            valid.append(i)\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDiGraphK_clean(data, sample_size, k=40, iters=20, name='ktemp'):\n",
    "    # cluster data using k-means\n",
    "    valid_idx = np.array(getOutliers(data['Lat'], data['Lon'], 1))\n",
    "    lat = np.array(data['Lat'])[valid_idx]\n",
    "    lon = np.array(data['Lon'])[valid_idx]\n",
    "    coordinates= np.array(list(zip(lat, lon)))\n",
    "    centroids, labels = kmeans2(whiten(coordinates), k, iter = iters) \n",
    "    num_points = len(lat)\n",
    "    # create graph with k nodes\n",
    "    G = nx.DiGraph(); \n",
    "    for pick_up in range(0,min(sample_size, num_points)):\n",
    "        \n",
    "        # get k-means centroid from src and randomize dest based on uniform distribution among all clusters\n",
    "        rand_idx = random.randint(0, num_points - 1)\n",
    "        la = lat[rand_idx]\n",
    "        lo = lon[rand_idx]\n",
    "        src = labels[rand_idx]\n",
    "        dest = random.randint(0, k - 1)\n",
    "        if (dest == src and src != 0):\n",
    "            dest = 0\n",
    "\n",
    "        # increase edge weight from src -->  dest by 1 \n",
    "        if not G.has_node(src):\n",
    "            G.add_node(src, lat=0, lon=0)\n",
    "        if not G.has_node(dest):\n",
    "            G.add_node(dest, lat=0, lon=0)\n",
    "        if not G.has_edge(src, dest):\n",
    "            G.add_edge(src, dest, weight=0)\n",
    "        G[src][dest]['weight'] += 1\n",
    "        \n",
    "        G.node[src]['lat'] = (G.node[src]['lat'] +  la) / 2 #keep a running average\n",
    "        G.node[src]['lon'] =  (G.node[src]['lon'] +  lo) / 2\n",
    "        \n",
    "        #print(\"Adding edge from \" + str(src) + \" to \" + str(dest))\n",
    "        if (pick_up > 1000 and pick_up % 1000 == 0):\n",
    "            nx.write_graphml(G, \"graphs/\" + name + \".graphml\")\n",
    "    return G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coords(G, pad=0.01):\n",
    "    # scale graph \n",
    "    node_lt = nx.get_node_attributes(G, 'lat').values()\n",
    "    node_ln = nx.get_node_attributes(G, 'lon').values()\n",
    "    node_lt = list(filter(lambda x: x != 0, node_lt)) # FILTER OUT ZEROS BC OF A BUG Idk\n",
    "    node_ln = list(filter(lambda x: x != 0, node_ln))\n",
    "    c2 = [min(node_lt)-pad, max(node_lt)+pad, min(node_ln)-pad, max(node_ln)+pad]\n",
    "    print(c2)\n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_graph_clean(G, coords=None, shapefiles={}, node_size=600): \n",
    "    plt.figure(figsize=(18,12))\n",
    "    if coords is None:\n",
    "        coords = get_coords(G)\n",
    "    m = Basemap(projection='merc',llcrnrlat=coords[0],urcrnrlat=coords[1],llcrnrlon=coords[2], urcrnrlon=coords[3], resolution='h')\n",
    "    m.drawcoastlines()\n",
    "    m.drawstates()\n",
    "    m.fillcontinents(color='blanchedalmond',lake_color='aqua')\n",
    "    #m = Basemap(projection='merc', width=10, height=10, lat_0=40.783058, lon_0=-73.971252)\n",
    "    pos = get_pos(G, m)\n",
    "    print(\"FINISHED PROJ. COORDINATES...STARTING NODES\")\n",
    "    nx.draw_networkx_nodes(G = G, pos = pos, node_color = 'b', alpha=0.8,node_size=get_weighted_nodes(G, node_size))\n",
    "    #nx.draw_networkx_edges(G = G, pos = pos, edge_color='white', alpha=0.8, arrows = True)\n",
    "    for file, name in shapefiles.items():\n",
    "        m.readshapefile(file, name)\n",
    "    plt.show()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = createDiGraphK_clean(data, len(data), k=40, iters=40, name='k_all_uber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = draw_graph_clean(g2, shapefiles=shapefiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok lit. but what do these places actually mean? I wish we had more k-clusters...we could try finding the optimal k value that would minimize the \"sum of squared distances\"...but is that what we actually want? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = draw_graph_clean(g2, shapefiles=shapefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine the optimal k-value \n",
    "def plot_k(data):\n",
    "    K = range(20, 100, 5)\n",
    "    inertia = []\n",
    "    km = KMeans()\n",
    "    for k in K:\n",
    "        coordinates= np.array(list(zip(data['Lat'][1:10000], data['Lon'][1:10000])))\n",
    "        km = KMeans(n_clusters=k)\n",
    "        k_model = km.fit(coordinates)\n",
    "        inertia.append(k_model.inertia_)\n",
    "        \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(K, inertia, 'b.')\n",
    "    plt.title(\"Inertia vs k\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.show()\n",
    "    return inertia\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = plot_k(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heyyyy so the \"elbow\" would be around 40-50, which is what we've been trying. nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay so trying to increase kvalues gives us more nodes to consider...but do we need that? idk, since there's no geographical relevance here so far. Also the edge weights rn are kinda visually meaningless so let's try to fix that -> maybe we can model CONGESTION by considering the amount of trips between each node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok well we are actually still unsure of the relationship between the out/in-degree distributions, so the edges right now are not as relevant. maybe let's focus on analyzing solely the pickup locations for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get  some colors\n",
    "def draw_graph_clean_colors(G, coords=None, shapefiles={}, node_size=600): \n",
    "    plt.figure(figsize=(18,12))\n",
    "    if coords is None:\n",
    "        coords = get_coords(G)\n",
    "    m = Basemap(projection='merc',llcrnrlat=coords[0],urcrnrlat=coords[1],llcrnrlon=coords[2], urcrnrlon=coords[3], resolution='h')\n",
    "    m.drawcoastlines()\n",
    "    m.drawstates()\n",
    "    m.fillcontinents(color='blanchedalmond',lake_color='aqua')\n",
    "    #m = Basemap(projection='merc', width=10, height=10, lat_0=40.783058, lon_0=-73.971252)\n",
    "    pos = get_pos(G, m)\n",
    "    \n",
    "    # create networkx graph \n",
    "    print(\"FINISHED PROJ. COORDINATES...STARTING NODES\")\n",
    "    cmap = plt.cm.Blues\n",
    "    nx.draw_networkx_nodes(G = G, pos = pos, node_color = get_weighted_nodes(G, 1.0), alpha=0.8,\n",
    "                           node_size=get_weighted_nodes(G, node_size), cmap=cmap)\n",
    "    #nx.draw_networkx_edges(G = G, pos = pos, edge_color='white', alpha=0.8, arrows = True)\n",
    "    for file, name in shapefiles.items():\n",
    "        m.readshapefile(file, name)\n",
    "        \n",
    "    # set colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm)\n",
    "    \n",
    "    plt.show()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kgraph = nx.read_graphml('graphs/k_all_uber.graphml')\n",
    "g4 = draw_graph_clean_colors(kgraph, shapefiles=shapefiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now time for taxi dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
